{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-19 14:25:14.799391: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:25:15.079765: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-19 14:25:15.079837: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-19 14:25:15.133120: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-19 14:25:15.241105: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-19 14:25:15.242447: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-19 14:25:16.323931: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Embedding, Flatten\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from langdetect import detect\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0        rating  preprocessed_review_length\n",
      "count  31246.000000  31246.000000                31246.000000\n",
      "mean   15907.414485      0.238174                   41.354733\n",
      "std     9259.086027      0.946370                   51.817764\n",
      "min        0.000000     -1.000000                    0.000000\n",
      "25%     7863.250000     -1.000000                   13.000000\n",
      "50%    15854.500000      1.000000                   25.000000\n",
      "75%    23924.750000      1.000000                   50.000000\n",
      "max    32035.000000      1.000000                 1528.000000\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "# give a description of the data\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "773/773 [==============================] - 215s 275ms/step - loss: 0.5483 - accuracy: 0.7933 - val_loss: 0.4501 - val_accuracy: 0.8345\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 207s 268ms/step - loss: 0.3671 - accuracy: 0.8754 - val_loss: 0.4658 - val_accuracy: 0.8312\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 205s 266ms/step - loss: 0.2829 - accuracy: 0.9039 - val_loss: 0.5370 - val_accuracy: 0.8241\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 206s 267ms/step - loss: 0.2288 - accuracy: 0.9215 - val_loss: 0.5760 - val_accuracy: 0.8029\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 218s 282ms/step - loss: 0.1954 - accuracy: 0.9348 - val_loss: 0.6405 - val_accuracy: 0.8040\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f08bb8ba8c0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "df = df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(df['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2\n",
    "\n",
    "# Tokenize the Arabic text\n",
    "tokenizer_arabic = Tokenizer()\n",
    "tokenizer_arabic.fit_on_texts(df['preprocessed_review'])\n",
    "sequences_arabic = tokenizer_arabic.texts_to_sequences(df['preprocessed_review'])\n",
    "padded_sequences_arabic = pad_sequences(sequences_arabic)\n",
    "\n",
    "# Define the LSTM model with dropout layers\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Embedding(input_dim=len(tokenizer_arabic.word_index) + 1, output_dim=100, input_length=padded_sequences_arabic.shape[1]))\n",
    "\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # Adding dropout to the LSTM layer\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))  # Adding dropout to the Dense layer\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))  # Three output nodes for negative, neutral, and positive\n",
    "\n",
    "# Compile the model with categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on Arabic data\n",
    "model.fit(padded_sequences_arabic, labels, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9239\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy of the model\n",
    "loss, accuracy = model.evaluate(padded_sequences_arabic, labels, verbose=False)\n",
    "\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 2s 72ms/step\n"
     ]
    }
   ],
   "source": [
    "# read the test data\n",
    "test_data = pd.read_csv('preprocessed_test.csv')\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the preprocessed reviews in the test data\n",
    "sequences_test = tokenizer_arabic.texts_to_sequences(test_data['preprocessed_review'])\n",
    "\n",
    "\n",
    "\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=padded_sequences_arabic.shape[1])\n",
    "\n",
    "# Predict the sentiment analysis using the trained model\n",
    "predictions = model.predict(padded_sequences_test)\n",
    "predicted_ratings = np.argmax(predictions, axis=1) - 1\n",
    "\n",
    "# Add the predicted ratings as a new column in the test data\n",
    "test_data['rating'] = predicted_ratings\n",
    "\n",
    "# Save the test data with the predicted ratings as a CSV file\n",
    "test_data.to_csv('predicted_test.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
