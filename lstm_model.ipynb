{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-18 21:03:13.768992: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 21:03:13.807958: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-18 21:03:13.807997: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-18 21:03:13.809347: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-18 21:03:13.815782: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-18 21:03:13.816578: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-18 21:03:14.552284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Embedding, Flatten\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from langdetect import detect\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Unnamed: 0        rating  preprocessed_review_length\n",
      "count  31246.000000  31246.000000                31246.000000\n",
      "mean   15907.414485      0.238174                   41.354733\n",
      "std     9259.086027      0.946370                   51.817764\n",
      "min        0.000000     -1.000000                    0.000000\n",
      "25%     7863.250000     -1.000000                   13.000000\n",
      "50%    15854.500000      1.000000                   25.000000\n",
      "75%    23924.750000      1.000000                   50.000000\n",
      "max    32035.000000      1.000000                 1528.000000\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "# give a description of the data\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "773/773 [==============================] - 100s 128ms/step - loss: 0.5235 - accuracy: 0.8020 - val_loss: 0.4551 - val_accuracy: 0.8307\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 94s 122ms/step - loss: 0.3509 - accuracy: 0.8763 - val_loss: 0.4670 - val_accuracy: 0.8314\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 96s 124ms/step - loss: 0.3088 - accuracy: 0.8968 - val_loss: 0.5158 - val_accuracy: 0.8223\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 95s 123ms/step - loss: 0.2151 - accuracy: 0.9288 - val_loss: 0.5717 - val_accuracy: 0.8209\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 94s 122ms/step - loss: 0.1752 - accuracy: 0.9425 - val_loss: 0.6397 - val_accuracy: 0.8112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f265fbbba60>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "df = df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(df['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2\n",
    "\n",
    "# Tokenize the Arabic text\n",
    "tokenizer_arabic = Tokenizer()\n",
    "tokenizer_arabic.fit_on_texts(df['preprocessed_review'])\n",
    "sequences_arabic = tokenizer_arabic.texts_to_sequences(df['preprocessed_review'])\n",
    "padded_sequences_arabic = pad_sequences(sequences_arabic)\n",
    "\n",
    "# Define the LSTM model with dropout layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer_arabic.word_index) + 1, output_dim=100, input_length=padded_sequences_arabic.shape[1]))\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2)) # Adding dropout to the LSTM layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Adding dropout to the Dense layer\n",
    "model.add(Dense(3, activation='softmax'))  # Three output nodes for negative, neutral, and positive\n",
    "\n",
    "# Compile the model with categorical_crossentropy\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model on Arabic data\n",
    "model.fit(padded_sequences_arabic, labels, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9284\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy of the model\n",
    "loss, accuracy = model.evaluate(padded_sequences_arabic, labels, verbose=False)\n",
    "\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will test the model on the test data\n",
    "# The submission file should be a .csv file that contains two columns: ID, and rating. The ID is the id of the Arabic sentences and the rating is the predicted sentiment analysis and should be one of the following values: 1,0,-1\n",
    "\n",
    "# read the test data\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# preprocess the test data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
