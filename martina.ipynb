{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]] [1. 0. 0.]\n",
      "Shapes - Padded Sequences: (30897, 100) Labels: (30897, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)        [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)     (None, 100, 128)             3003520   ['input_2[0][0]']             \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 100, 128)             131968    ['embedding_1[0][0]',         \n",
      " ltiHeadAttention)                                                   'embedding_1[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 100, 128)             0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 100, 128)             256       ['dropout_2[0][0]']           \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 128)                  0         ['layer_normalization_1[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 64)                   8256      ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 64)                   0         ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 3)                    195       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3144195 (11.99 MB)\n",
      "Trainable params: 3144195 (11.99 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "773/773 [==============================] - 58s 73ms/step - loss: 0.6541 - accuracy: 0.7305 - val_loss: 0.4771 - val_accuracy: 0.8257\n",
      "Epoch 2/5\n",
      "773/773 [==============================] - 59s 76ms/step - loss: 0.4448 - accuracy: 0.8546 - val_loss: 0.5031 - val_accuracy: 0.8304\n",
      "Epoch 3/5\n",
      "773/773 [==============================] - 53s 68ms/step - loss: 0.3553 - accuracy: 0.8842 - val_loss: 0.5386 - val_accuracy: 0.8244\n",
      "Epoch 4/5\n",
      "773/773 [==============================] - 52s 68ms/step - loss: 0.2908 - accuracy: 0.9022 - val_loss: 0.6084 - val_accuracy: 0.8170\n",
      "Epoch 5/5\n",
      "773/773 [==============================] - 54s 69ms/step - loss: 0.2483 - accuracy: 0.9150 - val_loss: 0.7451 - val_accuracy: 0.8128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7fc5b9fa95a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, MultiHeadAttention, LayerNormalization, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "df = df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(df['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2\n",
    "print(labels, labels[0])\n",
    "# Tokenize the Arabic text\n",
    "tokenizer_arabic = Tokenizer()\n",
    "tokenizer_arabic.fit_on_texts(df['preprocessed_review'])\n",
    "sequences_arabic = tokenizer_arabic.texts_to_sequences(df['preprocessed_review'])\n",
    "max_sequence_length = 100  # Set your desired sequence length\n",
    "padded_sequences_arabic = pad_sequences(sequences_arabic, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "def transformer_classifier(max_sequence_length, vocab_size, num_classes):\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=128)(inputs)\n",
    "    \n",
    "    # Transformer layers - You can use TensorFlow's MultiHeadAttention and Transformer layers\n",
    "    transformer_layer = MultiHeadAttention(num_heads=2, key_dim=128)(embedding, embedding)\n",
    "    transformer_layer = Dropout(0.2)(transformer_layer)\n",
    "    transformer_layer = LayerNormalization(epsilon=1e-6)(transformer_layer)\n",
    "    transformer_layer = GlobalAveragePooling1D()(transformer_layer)\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    dense = Dense(64, activation='relu')(transformer_layer)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Assuming max_sequence_length, vocab_size, and num_classes are defined appropriately\n",
    "max_sequence_length = 100  # Example sequence length\n",
    "vocab_size = 10000  # Example vocabulary size\n",
    "num_classes = 3  # Example number of classes\n",
    "\n",
    "# Assuming padded_sequences_arabic and labels are prepared as before\n",
    "print(\"Shapes - Padded Sequences:\", padded_sequences_arabic.shape, \"Labels:\", labels.shape)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer_arabic.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "# Create the transformer model for text classification\n",
    "model = transformer_classifier(max_sequence_length, vocab_size, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_arabic, labels, epochs=5, batch_size=32, validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
