{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import emoji\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from qalsadi import lemmatizer\n",
    "from googletrans import Translator\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'train.xlsx'\n",
    "train = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nulls_and_duplicates(df):\n",
    "    df_cleaned = df.dropna()\n",
    "    df_cleaned = df_cleaned.drop_duplicates()\n",
    "    return df_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    return re.sub('\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(r\"[ًًٌٍَُِّْ]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"[يى]\", \"ي\", text) \n",
    "    text = re.sub(\"[ؤئ]\", \"ء\", text) \n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ـ\", \"\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    # Remove 3+ repeated consecutive characters\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_long_words(text, threshold=15):\n",
    "    return ' '.join(word for word in text.split(\" \") if len(word) < threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic_words(text):\n",
    "    \"\"\"\n",
    "    Source: https://gist.github.com/mohabmes/33b724edfd4f0f3ec2e6644168db516e#file-preprocess_arabic_text-py-L22\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\u0600-\\u06ff\\u0750-\\u077f\\ufb50-\\ufbc1\\ufbd3-\\ufd3f\\ufd50-\\ufd8f\\ufd50-\\ufd8f\\ufe70-\\ufefc\\uFDF0-\\uFDFD]+', ' ', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = set(stopwords.words(\"english\")+stopwords.words(\"arabic\"))\n",
    "words_to_keep = {'not', 'لا', 'ليس', 'مش', 'لم', 'لن', 'جدا', 'اكثر', 'قليل', 'كثير', 'حب', 'بكره', 'بحب', 'عجب', 'غير', 'كره'}\n",
    "stopwords_list = stopwords_list.difference(words_to_keep)\n",
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stopwords_list])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source:\n",
    "https://github.com/a-ibrahimi/Arabic-Emojipedia\n",
    "https://stackoverflow.com/a/76419165/13218954\n",
    "\"\"\"\n",
    "def build_emoji_dictionary():\n",
    "    csv_file = 'emojis.csv'\n",
    "    emoji_dict = {}\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            emoji = row[0]\n",
    "            text = row[1]\n",
    "            emoji_dict[emoji] = text\n",
    "    return emoji_dict\n",
    "\n",
    "def replace_emojis(text):\n",
    "    emoji_dict = build_emoji_dictionary()\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    for emo in emojis:\n",
    "        if emo['emoji'] in emoji_dict:\n",
    "            # Replace the emoji with the corresponding text surrounded by spaces\n",
    "            text = text.replace(emo['emoji'], ' ' + emoji_dict[emo['emoji']] + ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate non-Arabic words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator()\n",
    "# transliterator = FrancoArabicTransliterator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_lemmatizer = lemmatizer.Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    try:\n",
    "        language = translator.detect(word).lang\n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "    if language == 'en':\n",
    "        try:\n",
    "            translated_word = translator.translate(word, src='en', dest='ar').text\n",
    "            if translated_word in stopwords_list:\n",
    "                # to be removed later\n",
    "                return word\n",
    "            word = translated_word\n",
    "        except:\n",
    "            return word\n",
    "\n",
    "    try:\n",
    "        lemma = arabic_lemmatizer.lemmatize(word)\n",
    "        return lemma\n",
    "    except:\n",
    "        return word\n",
    "\n",
    "def lemmatize_multilingual_text(text):\n",
    "    lemmatized_words = [lemmatize_word(word) for word in text.split()]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = str(text).lower()\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_long_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    text = replace_emojis(text)\n",
    "    # Remove unhandled emojis/invalid characters\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    # Collapse any consecutive spaces to a single space\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = lemmatize_multilingual_text(text)\n",
    "    # Remove any not-translated non-arabic word\n",
    "    text = remove_non_arabic_words(text)\n",
    "    # Normalize again as some words have inconcsistent data from lemmatization  \n",
    "    text = remove_diacritics(text)\n",
    "    text = normalize_arabic(text)   \n",
    "    text = remove_stopwords(text)\n",
    " \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # utf-8 encoding\n",
    "    df['preprocessed_review'] = df['review_description'].apply(lambda x: x.encode('utf-8').decode('utf-8'))\n",
    "    # convert it to string\n",
    "    df['preprocessed_review'] = df['preprocessed_review'].astype(str)\n",
    "    \n",
    "    df = remove_nulls_and_duplicates(df)\n",
    "    df['preprocessed_review'] = df['preprocessed_review'].apply(preprocess)\n",
    "    # A review may become empty after removing stopwords\n",
    "    df = remove_nulls_and_duplicates(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = clean_df(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = clean_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.to_csv(\"preprocessed_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
