{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/madboly/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/madboly/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from arabic_reshaper import ArabicReshaper\n",
    "from bidi.algorithm import get_display\n",
    "import nltk\n",
    "import emoji\n",
    "import csv\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from qalsadi import lemmatizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/madboly/arabic-sentiment-analysis/preprocessing_test.ipynb Cell 4\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/madboly/arabic-sentiment-analysis/preprocessing_test.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m file_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest.csv\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/madboly/arabic-sentiment-analysis/preprocessing_test.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m data \u001b[39m=\u001b[39m file_path\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/madboly/arabic-sentiment-analysis/preprocessing_test.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mreview_description\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mreview_description\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mencode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/madboly/arabic-sentiment-analysis/preprocessing_test.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mreview_description\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mreview_description\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mastype(\u001b[39mstr\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "file_path = 'train.xlsx'\n",
    "data = pd.read_excel(file_path)\n",
    "data['review_description'] = data['review_description'].apply(lambda x: x.encode('utf-8').decode('utf-8'))\n",
    "data['review_description'] = data['review_description'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "source": [
    "##### Lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['preprocessed_review'] = data['review_description'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dealing with duplicated reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop_duplicates(subset='preprocessed_review')\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "    text = re.sub('[%s]' % re.escape(punctuations), ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_digits(text):\n",
    "    return re.sub('\\d+', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_diacritics(text):\n",
    "    return re.sub(r\"[ًًٌٍَُِّْ]\", \"\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"[يى]\", \"ي\", text) \n",
    "    text = re.sub(\"[ؤئ]\", \"ء\", text) \n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"ـ\", \"\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_repeating_char(text):\n",
    "    # Remove 3+ repeated consecutive characters\n",
    "    return re.sub(r'(.)\\1{2,}', r'\\1', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_long_words(text, threshold=15):\n",
    "    return ' '.join(word for word in text.split(\" \") if len(word) < threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words(\"english\")+stopwords.words(\"arabic\")\n",
    "len(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    text = ' '.join([word for word in word_tokenize(text) if word not in stopwords_list])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Source:\n",
    "https://github.com/a-ibrahimi/Arabic-Emojipedia\n",
    "https://stackoverflow.com/a/76419165/13218954\n",
    "\"\"\"\n",
    "def build_emoji_dictionary():\n",
    "    csv_file = 'emojis.csv'\n",
    "    emoji_dict = {}\n",
    "    with open(csv_file, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            emoji = row[0]\n",
    "            text = row[1]\n",
    "            emoji_dict[emoji] = text\n",
    "    return emoji_dict\n",
    "\n",
    "def replace_emojis(text):\n",
    "    emoji_dict = build_emoji_dictionary()\n",
    "    emojis = emoji.emoji_list(text)\n",
    "    for emo in emojis:\n",
    "        if emo['emoji'] in emoji_dict:\n",
    "            # Replace the emoji with the corresponding text surrounded by spaces\n",
    "            text = text.replace(emo['emoji'], ' ' + emoji_dict[emo['emoji']] + ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "arabic_lemmatizer = lemmatizer.Lemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_word(word):\n",
    "    try:\n",
    "        lang = detect(word)\n",
    "    except:\n",
    "        return word\n",
    "    \n",
    "    if lang == 'en':\n",
    "        return wordnet_lemmatizer.lemmatize(word)\n",
    "    elif lang == 'ar':\n",
    "        return arabic_lemmatizer.lemmatize(word)\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def lemmatize_multilingual_text(text):\n",
    "    lemmatized_words = [lemmatize_word(word) for word in text.split()]\n",
    "    return ' '.join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_digits(text)\n",
    "    text = remove_diacritics(text)\n",
    "    text = normalize_arabic(text)\n",
    "    text = remove_repeating_char(text)\n",
    "    text = remove_long_words(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = replace_emojis(text)\n",
    "    # Remove unhandled emojis/invalid characters\n",
    "    text = re.sub(r'[^\\w\\s]','', text)\n",
    "    # Collapse any consecutive spaces to a single space\n",
    "    text = re.sub('\\s+', ' ', text).strip()\n",
    "    text = lemmatize_multilingual_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['preprocessed_review'] = data['preprocessed_review'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a feature\n",
    "data['preprocessed_review_length'] = data['preprocessed_review'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check duplicates and nulls again (if a review became empty)\n",
    "print(data['preprocessed_review'].isnull().sum())\n",
    "print(data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"preprocessed_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Distribution\n",
    "sentiment_counts = data['rating'].value_counts()\n",
    "\n",
    "# Review Length Analysis\n",
    "data['review_length'] = data['review_description'].apply(len)\n",
    "\n",
    "# Plotting the sentiment distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=sentiment_counts.index, y=sentiment_counts.values)\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.xticks(ticks=[0, 1, 2], labels=['Negative (-1)', 'Neutral (0)', 'Positive (1)'])\n",
    "plt.show()\n",
    "\n",
    "# Plotting the distribution of review lengths\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.histplot(data['review_length'], bins=50)\n",
    "plt.title('Review Length Distribution')\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['review_length'].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
