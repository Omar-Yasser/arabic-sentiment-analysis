{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Transformer-architecture](https://github.com/greyhatguy007/deep-learning-specialization/blob/main/C5-sequence-models/week4/C5W4A1/transformer.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.mdpi.com/mathematics/mathematics-11-04960/article_deploy/html/images/mathematics-11-04960-g001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import Embedding, MultiHeadAttention, Dense, Input, Dropout, LayerNormalization\n",
    "from transformers import DistilBertTokenizerFast #, TFDistilBertModel\n",
    "from transformers import TFDistilBertForTokenClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position,k,d):\n",
    "    '''\n",
    "    position : col vec (positions)\n",
    "    k : row vec (dimention span)\n",
    "    d : int (encoder size)\n",
    "\n",
    "    Return :-\n",
    "    angles : (position, d) np.array\n",
    "    '''\n",
    "    i = k//2\n",
    "    angles = position / np.power(10000, 2 * i / d)\n",
    "    \n",
    "    return angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d): # sin => i even , cos => i odd\n",
    "    '''\n",
    "    positions : int (max num of positions to be encoded)\n",
    "    d : int (encoder size)\n",
    "\n",
    "    Return :-\n",
    "    pos_encoding : (1, position, d_model) matrix\n",
    "    '''\n",
    "    angles = get_angles(np.arange(positions)[:, np.newaxis],\n",
    "                        np.arange(d)[np.newaxis, :],\n",
    "                        d)\n",
    "    \n",
    "    # even -> sin\n",
    "    angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "\n",
    "    # odd -> cos\n",
    "    angles[:, 0::1] = np.sin(angles[:, 0::1])\n",
    "\n",
    "    pos_encoding = angles[np.newaxis, ...] # The ... is a placeholder that indicates that all the existing axes from the angles array should be retained.\n",
    "\n",
    "    return tf.cast(pos_encoding ,dtype=tf.float32) #to convert datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(decoder_ids):\n",
    "    '''\n",
    "    decoder_ids : matrix (n,m)\n",
    "\n",
    "    Return :-\n",
    "    mask : (n, 1, m) binary tensor\n",
    "    '''\n",
    "    seq = 1 - tf.cast(tf.math.equal(decoder_ids, 0), tf.float32)\n",
    "\n",
    "    seq = seq[:, np.newaxis, :] # padding 0\n",
    "\n",
    "    # to pad -inf -> x + (1 - padding_mask(x)) * -1.0e9)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The look-ahead mask helps your model pretend that it correctly predicted a part of the output and see if, without looking ahead, it can correctly predict the next output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_ahead_mask(seq_len):\n",
    "    '''\n",
    "    seq_len : matrix size\n",
    "\n",
    "    Return :-\n",
    "    mask : (size, size) tensor => lower triangular matrix filled with ones\n",
    "    '''\n",
    "    mask = tf.linalg.band_part(tf.ones((1, seq_len, seq_len)), -1, 0)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention(Q, K, V)= *softmax( [ (Q * K^T) / sqrt(dk) ] + M )* * *V*\n",
    "\n",
    "*Q* is the matrix of queries\n",
    "\n",
    "*K* is the matrix of keys\n",
    "\n",
    "*V* is the matrix of values\n",
    "\n",
    "*M* is the optional mask you choose to apply\n",
    "\n",
    "*dk* is the dimension of the keys, which is used to scale everything down so the softmax doesn't explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    '''\n",
    "    q : query shape == (..., seq_len_q, depth)\n",
    "    k : key shape == (..., seq_len_k, depth)\n",
    "    v : value shape == (..., seq_len_v, depth_v)\n",
    "    mask: Float tensor with shape broadcastable to (..., seq_len_q, seq_len_k).\n",
    "\n",
    "    Return :-\n",
    "    attention ,attention_weights\n",
    "    '''\n",
    "\n",
    "    qk = tf.matmul(q, k, transpose_b = True) # matrix_multiplication\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "\n",
    "    scaled_attention = qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is None:\n",
    "        scaled_attention += ((1 - mask) * -1.0e9)\n",
    "    \n",
    "    attention_weights = tf.nn.softmax(scaled_attention, axis= -1)\n",
    "    attention = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return attention, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, \n",
    "                 dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "\n",
    "\n",
    "    def fit(self, x, training, mask):\n",
    "        '''\n",
    "        x : tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        training : boolean, set to true to activate the training mode for dropout layers\n",
    "        mask : boolean mask to ensure that the padding is not treated as part of the input\n",
    "\n",
    "        Return :-\n",
    "        encoder_output : tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        '''\n",
    "\n",
    "        attention_output, attention_weights = self.multi_head_attention(x, x, x, mask)\n",
    "\n",
    "        output1 = self.layernorm1(x + attention_output)\n",
    "\n",
    "        ffn_output =self.ffn(output1)\n",
    "        ffn_output = self.dropout_ffn(ffn_output, training=training)\n",
    "\n",
    "        encoder_output = self.layernorm2(output1 + ffn_output)\n",
    "\n",
    "        return encoder_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "                 maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.positional_encoding = positional_encoding(maximum_position_encoding, \n",
    "                                                self.embedding_dim)\n",
    "\n",
    "\n",
    "        self.encoder_layers = [EncoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        \n",
    "    def fit(self, x, training, mask):\n",
    "        # x : tensor of shape (batch_size, input_seq_len)\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training= training)\n",
    "\n",
    "        attention_weights = []\n",
    "        for i in range(self.num_layers):\n",
    "            x, w = self.encoder_layers[i](x, training, mask)\n",
    "            attention_weights.append(w)\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, fully_connected_dim, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.multi_head_attention1 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.multi_head_attention2 = MultiHeadAttention(num_heads=num_heads,\n",
    "                                      key_dim=embedding_dim,\n",
    "                                      dropout=dropout_rate)\n",
    "\n",
    "        self.ffn = FullyConnected(embedding_dim=embedding_dim,\n",
    "                                  fully_connected_dim=fully_connected_dim)\n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=layernorm_eps)\n",
    "        self.layernorm3 = LayerNormalization(epsilon=layernorm_eps)\n",
    "\n",
    "        self.dropout_ffn = Dropout(dropout_rate)\n",
    "    \n",
    "    def fit(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
    "        \n",
    "        # BLOCK 1\n",
    "\n",
    "        multi_attention_output1, attention_weights_block1 = self.multi_head_attention1(x, x, x, look_ahead_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        q1 = self.layernorm1(multi_attention_output1 + x)\n",
    "        \n",
    "        # BLOCK 2\n",
    "\n",
    "        multi_attention_output2, attention_weights_block2 = self.multi_head_attention2(q1, encoder_output, encoder_output, padding_mask, return_attention_scores=True)  # (batch_size, target_seq_len, d_model)\n",
    "        multi_attention_output2 = self.layernorm2(multi_attention_output2 + q1)\n",
    "\n",
    "        # BLOCK 3\n",
    "\n",
    "        fully_connected_output = self.ffn(multi_attention_output2)\n",
    "        fully_connected_output = self.dropout_ffn(fully_connected_output, training= training)\n",
    "\n",
    "        output3 = self.layernorm3(fully_connected_output + multi_attention_output2)\n",
    "\n",
    "        return output3, attention_weights_block1, attention_weights_block2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size,\n",
    "               maximum_position_encoding, dropout_rate=0.1, layernorm_eps=1e-6):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = Embedding(input_vocab_size, self.embedding_dim)\n",
    "        self.positional_encoding = positional_encoding(maximum_position_encoding, self.embedding_dim)\n",
    "\n",
    "        self.decoder_layers = [DecoderLayer(embedding_dim=self.embedding_dim,\n",
    "                                        num_heads=num_heads,\n",
    "                                        fully_connected_dim=fully_connected_dim,\n",
    "                                        dropout_rate=dropout_rate,\n",
    "                                        layernorm_eps=layernorm_eps) \n",
    "                           for _ in range(self.num_layers)]\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "    \n",
    "    def fit(self, x, encoder_output, training, look_ahead_mask, padding_mask):\n",
    "        # x : tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "        seq_len = tf.shape(x)[-1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.embedding_dim, tf.float32))\n",
    "\n",
    "        # Add the position encoding to embedding\n",
    "        x += self.positional_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training= training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, attention_weights_block1, attention_weights_block2 = self.decoder_layers[i](x, \n",
    "                                                                                           encoder_output,\n",
    "                                                                                           training,\n",
    "                                                                                           look_ahead_mask,\n",
    "                                                                                           padding_mask)\n",
    "            #update attention_weights dictionary with the attention weights of block 1 and block 2\n",
    "            attention_weights['decoder_layer{}_block1_self_att'.format(i+1)] = attention_weights_block1\n",
    "            attention_weights['decoder_layer{}_block2_decenc_att'.format(i+1)] = attention_weights_block2\n",
    "\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, embedding_dim, num_heads, fully_connected_dim, input_vocab_size, \n",
    "                max_positional_encoding, dropout_rate=0.1, layernorm_eps=1e-6, pad_id=0):\n",
    "        \n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.pad_id = pad_id\n",
    "        self.positional_encoding = positional_encoding(max_positional_encoding + 1, embedding_dim)\n",
    "        \n",
    "\n",
    "        self.encoder = Encoder(num_layers=num_layers,\n",
    "                               embedding_dim=embedding_dim,\n",
    "                               num_heads=num_heads,\n",
    "                               fully_connected_dim=fully_connected_dim,\n",
    "                               input_vocab_size=input_vocab_size,\n",
    "                               maximum_position_encoding=max_positional_encoding,\n",
    "                               dropout_rate=dropout_rate,\n",
    "                               layernorm_eps=layernorm_eps)\n",
    "\n",
    "        # self.decoder = Decoder(num_layers=num_layers, \n",
    "        #                        embedding_dim=embedding_dim,\n",
    "        #                        num_heads=num_heads,\n",
    "        #                        fully_connected_dim=fully_connected_dim,\n",
    "        #                        input_vocab_size=input_vocab_size, \n",
    "        #                        maximum_position_encoding=max_positional_encoding,\n",
    "        #                        dropout_rate=dropout_rate,\n",
    "        #                        layernorm_eps=layernorm_eps)\n",
    "\n",
    "        # layers to classify\n",
    "        self.linear = tf.nn.Linear(embedding_dim, 3)\n",
    "        self.softmax = tf.nn.Softmax(dim=-1)\n",
    "        #self.final_layer = Dense(input_vocab_size, activation='softmax')\n",
    "\n",
    "    def fit(self, input_sentence, training, encoder_padding_mask, look_ahead_mask=False, decoder_padding_mask=False):\n",
    "        '''\n",
    "        input_sentence : tensor of shape (batch_size, input_seq_len, fully_connected_dim)\n",
    "                              an array of the indexes of the words in the input sentence\n",
    "        \n",
    "        training : boolean, set to true to activate the training mode for dropout layers\n",
    "        \n",
    "        enc_padding_mask : boolean mask to ensure that the padding is not treated as part of the input\n",
    "        \n",
    "        look_ahead_mask : boolean mask for the target_input\n",
    "        \n",
    "        padding_mask : boolean mask for the second multihead attention layer\n",
    "\n",
    "        Return :-\n",
    "\n",
    "        final_output\n",
    "        attention_weights - Dictionary of tensors containing all the attention weights for the decoder\n",
    "                                each of shape Tensor of shape (batch_size, num_heads, target_seq_len, input_seq_len)\n",
    "        '''\n",
    "        seq_length = input_sentence.shape[1]\n",
    "        batch_size = tf.shape(input_sentence)[0]\n",
    "\n",
    "        # Repeat the positions along the batch dimension\n",
    "        position_pad_mask = padding_mask(input_sentence)\n",
    "        position_values = tf.range(tf.shape(input_sentence)[1]) + 1  \n",
    "        positions = tf.where(position_pad_mask, tf.zeros_like(position_values), tf.cast(position_values, dtype=tf.float32))\n",
    "\n",
    "        outputs = self.embedding(input_sentence) + self.pos_embedding(positions)\n",
    "\n",
    "        encoder_output, attention_weights = self.encoder(input_sentence, training, encoder_padding_mask)# (batch_size, inp_seq_len, fully_connected_dim)\n",
    "        \n",
    "        #decoder_output, attention_weights = self.decoder(input_sentence, encoder_output, training, look_ahead_mask, decoder_padding_mask)\n",
    "\n",
    "        #final_output = self.final_layer(decoder_output)\n",
    "        max_values = tf.math.reduce_max(encoder_output, axis=1)\n",
    "        indices = tf.argmax(encoder_output, axis=1)\n",
    "\n",
    "        final_output = self.softmax(self.linear(max_values))\n",
    "\n",
    "        return final_output, attention_weights\n",
    "    \n",
    "\n",
    "    def get_attention_padding_mask(self, q, k, pad_id):\n",
    "        attn_pad_mask = k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)\n",
    "        # |attn_pad_mask| : (batch_size, q_len, k_len)\n",
    "\n",
    "        return attn_pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 31246 entries, 0 to 31245\n",
      "Data columns (total 5 columns):\n",
      " #   Column                      Non-Null Count  Dtype \n",
      "---  ------                      --------------  ----- \n",
      " 0   Unnamed: 0                  31246 non-null  int64 \n",
      " 1   review_description          31246 non-null  object\n",
      " 2   rating                      31246 non-null  int64 \n",
      " 3   preprocessed_review         30897 non-null  object\n",
      " 4   preprocessed_review_length  31246 non-null  int64 \n",
      "dtypes: int64(3), object(2)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"preprocessed_train.csv\")\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>review_description</th>\n",
       "      <th>rating</th>\n",
       "      <th>preprocessed_review</th>\n",
       "      <th>preprocessed_review_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...</td>\n",
       "      <td>-1</td>\n",
       "      <td>شرك زباله سواقين بتبرشم مفيش حت رقم للشكاوي سو...</td>\n",
       "      <td>130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...</td>\n",
       "      <td>1</td>\n",
       "      <td>خدم دفع طريق كي نت توقف عند صبح فقط دفع نقدا</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...</td>\n",
       "      <td>-1</td>\n",
       "      <td>تطبيق غب جاري حذف عامل اكواد خصم استخدم اكترى ...</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...</td>\n",
       "      <td>1</td>\n",
       "      <td>علا تطبيق ممتاز امكانيه أتاح مستخدم تطبيق ان ا...</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...</td>\n",
       "      <td>-1</td>\n",
       "      <td>سيء جدا اسعار رسوم توصيل أمات واقع صل</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                 review_description  rating  \\\n",
       "0           0  شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...      -1   \n",
       "1           1  خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...       1   \n",
       "2           2  تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...      -1   \n",
       "3           3  فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...       1   \n",
       "4           4  سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...      -1   \n",
       "\n",
       "                                 preprocessed_review  \\\n",
       "0  شرك زباله سواقين بتبرشم مفيش حت رقم للشكاوي سو...   \n",
       "1       خدم دفع طريق كي نت توقف عند صبح فقط دفع نقدا   \n",
       "2  تطبيق غب جاري حذف عامل اكواد خصم استخدم اكترى ...   \n",
       "3  علا تطبيق ممتاز امكانيه أتاح مستخدم تطبيق ان ا...   \n",
       "4              سيء جدا اسعار رسوم توصيل أمات واقع صل   \n",
       "\n",
       "   preprocessed_review_length  \n",
       "0                         130  \n",
       "1                          44  \n",
       "2                         216  \n",
       "3                          87  \n",
       "4                          37  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "data = data.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(data['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['preprocessed_review'])\n",
    "sequences_arabic = tokenizer.texts_to_sequences(data['preprocessed_review'])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transformer = Transformer(num_layers = 10, \n",
    "                        embedding_dim = 512, \n",
    "                        num_heads = 8, \n",
    "                        fully_connected_dim = 32, \n",
    "                        input_vocab_size = vocab_size, \n",
    "                        max_positional_encoding_input = 512)\n",
    "\n",
    "training = True\n",
    "encoder_padding_mask = \n",
    "model = transformer(sequences_arabic, training, encoder_padding_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]] [1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "df = df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(df['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2\n",
    "print(labels, labels[0])\n",
    "# Tokenize the Arabic text\n",
    "tokenizer_arabic = Tokenizer()\n",
    "tokenizer_arabic.fit_on_texts(df['preprocessed_review'])\n",
    "sequences_arabic = tokenizer_arabic.texts_to_sequences(df['preprocessed_review'])\n",
    "max_sequence_length = 100  # Set your desired sequence length\n",
    "padded_sequences_arabic = pad_sequences(sequences_arabic, maxlen=max_sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30897, 100)\n"
     ]
    }
   ],
   "source": [
    "print(padded_sequences_arabic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_sequence_length, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.pos_encoding = self.positional_encoding()\n",
    "\n",
    "    def get_angles(self, position, i):\n",
    "        angle_rates = 1 / tf.pow(10000, tf.cast(2 * (i // 2) / self.embedding_dim, dtype=tf.float32))\n",
    "        return tf.cast(position, dtype=tf.float32) * angle_rates\n",
    "\n",
    "    def positional_encoding(self):\n",
    "        angle_rads = self.get_angles(tf.range(self.max_sequence_length)[:, tf.newaxis],\n",
    "                                     tf.range(self.embedding_dim)[tf.newaxis, :])\n",
    "\n",
    "        # Apply sine to even indices in the array\n",
    "        pos_encoding = tf.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # Apply cosine to odd indices in the array\n",
    "        pos_encoding = tf.concat([pos_encoding, tf.cos(angle_rads[:, 1::2])], axis=-1)\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return pos_encoding\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.cast(inputs, dtype=tf.float32)\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]] [1. 0. 0.]\n",
      "Shapes - Padded Sequences: (30897, 100) Labels: (30897, 3)\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)       [(None, 100)]                0         []                            \n",
      "                                                                                                  \n",
      " embedding_12 (Embedding)    (None, 100, 128)             3003520   ['input_13[0][0]']            \n",
      "                                                                                                  \n",
      " positional_encoding_7 (Pos  (None, 100, 128)             0         ['embedding_12[0][0]']        \n",
      " itionalEncoding)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, 100, 128)             131968    ['positional_encoding_7[0][0]'\n",
      " ltiHeadAttention)                                                  , 'positional_encoding_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, 100, 128)             0         ['multi_head_attention_1[0][0]\n",
      " OpLambda)                                                          ',                            \n",
      "                                                                     'positional_encoding_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, 100, 128)             256       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " sequential_1 (Sequential)   (None, 100, 128)             8352      ['layer_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 100, 128)             0         ['sequential_1[0][0]']        \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, 100, 128)             0         ['layer_normalization_2[0][0]'\n",
      " OpLambda)                                                          , 'dropout_2[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, 100, 128)             256       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling1d_1  (None, 128)                  0         ['layer_normalization_3[0][0]'\n",
      "  (GlobalAveragePooling1D)                                          ]                             \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 64)                   8256      ['global_average_pooling1d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 64)                   0         ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 3)                    195       ['dropout_3[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3152803 (12.03 MB)\n",
      "Trainable params: 3152803 (12.03 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "3090/3090 [==============================] - 112s 34ms/step - loss: 0.7501 - auc_1: 0.7177 - accuracy: 0.6612 - val_loss: 0.5141 - val_auc_1: 0.8813 - val_accuracy: 0.8113\n",
      "Epoch 2/10\n",
      "3090/3090 [==============================] - 114s 37ms/step - loss: 0.5084 - auc_1: 0.8851 - accuracy: 0.8248 - val_loss: 0.4783 - val_auc_1: 0.8978 - val_accuracy: 0.8244\n",
      "Epoch 3/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.4500 - auc_1: 0.9083 - accuracy: 0.8506 - val_loss: 0.4713 - val_auc_1: 0.9028 - val_accuracy: 0.8322\n",
      "Epoch 4/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.4013 - auc_1: 0.9250 - accuracy: 0.8732 - val_loss: 0.4911 - val_auc_1: 0.8988 - val_accuracy: 0.8218\n",
      "Epoch 5/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.3568 - auc_1: 0.9391 - accuracy: 0.8884 - val_loss: 0.5434 - val_auc_1: 0.8930 - val_accuracy: 0.8141\n",
      "Epoch 6/10\n",
      "3090/3090 [==============================] - 103s 33ms/step - loss: 0.3214 - auc_1: 0.9491 - accuracy: 0.8978 - val_loss: 0.5393 - val_auc_1: 0.8869 - val_accuracy: 0.8133\n",
      "Epoch 7/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.2942 - auc_1: 0.9571 - accuracy: 0.9047 - val_loss: 0.5699 - val_auc_1: 0.8843 - val_accuracy: 0.8118\n",
      "Epoch 8/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.2723 - auc_1: 0.9623 - accuracy: 0.9114 - val_loss: 0.5967 - val_auc_1: 0.8799 - val_accuracy: 0.7994\n",
      "Epoch 9/10\n",
      "3090/3090 [==============================] - 104s 34ms/step - loss: 0.2553 - auc_1: 0.9662 - accuracy: 0.9164 - val_loss: 0.6253 - val_auc_1: 0.8691 - val_accuracy: 0.8005\n",
      "Epoch 10/10\n",
      "3090/3090 [==============================] - 105s 34ms/step - loss: 0.2395 - auc_1: 0.9697 - accuracy: 0.9212 - val_loss: 0.6653 - val_auc_1: 0.8624 - val_accuracy: 0.7917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x21b0e0fd610>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import auc\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, MultiHeadAttention, LayerNormalization, Dense, Dropout, Embedding, GlobalAveragePooling1D\n",
    "import tensorflow_addons as tfa\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.metrics import AUC\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('preprocessed_train.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "df = df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Convert ratings to one-hot encoded labels\n",
    "labels = to_categorical(df['rating'] + 1)  # Adding 1 to convert -1, 0, 1 to 0, 1, 2\n",
    "print(labels, labels[0])\n",
    "# Tokenize the Arabic text\n",
    "tokenizer_arabic = Tokenizer()\n",
    "tokenizer_arabic.fit_on_texts(df['preprocessed_review'])\n",
    "sequences_arabic = tokenizer_arabic.texts_to_sequences(df['preprocessed_review'])\n",
    "max_sequence_length = 100  # Set your desired sequence length\n",
    "padded_sequences_arabic = pad_sequences(sequences_arabic, maxlen=max_sequence_length)\n",
    "\n",
    "def FullyConnected(embedding_dim, fully_connected_dim):\n",
    "    return tf.keras.Sequential([\n",
    "        Dense(fully_connected_dim, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        Dense(embedding_dim)  # (batch_size, seq_len, d_model)\n",
    "    ])\n",
    "\n",
    "def transformer_classifier(max_sequence_length, vocab_size, num_classes):\n",
    "    # Input for variable-length sequences of integers\n",
    "    inputs = Input(shape=(max_sequence_length,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=128 ,input_length=padded_sequences_arabic.shape[1])(inputs)\n",
    "    positional_encoding = PositionalEncoding(max_sequence_length, 128)(embedding)\n",
    "\n",
    "    # Transformer layers - You can use TensorFlow's MultiHeadAttention and Transformer layers\n",
    "    mha_layer = MultiHeadAttention(num_heads=2, key_dim=128)(positional_encoding, positional_encoding)\n",
    "    norm1_layer = LayerNormalization(epsilon=1e-6)(mha_layer + positional_encoding)\n",
    "    ffn_layer = FullyConnected(embedding_dim= 128, fully_connected_dim= 32)(norm1_layer)\n",
    "    ffn_layer = Dropout(0.2)(ffn_layer)\n",
    "    norm2_layer = LayerNormalization(epsilon=1e-6)(norm1_layer + ffn_layer)\n",
    "    transformer_layer = GlobalAveragePooling1D()(norm2_layer)\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    dense = Dense(64, activation='relu')(transformer_layer)\n",
    "    dropout = Dropout(0.5)(dense)\n",
    "    outputs = Dense(num_classes, activation='softmax')(dropout)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "# Assuming max_sequence_length, vocab_size, and num_classes are defined appropriately\n",
    "max_sequence_length = 100  \n",
    "vocab_size = len(tokenizer_arabic.word_index) + 1  \n",
    "num_classes = 3  \n",
    "\n",
    "# Assuming padded_sequences_arabic and labels are prepared as before\n",
    "print(\"Shapes - Padded Sequences:\", padded_sequences_arabic.shape, \"Labels:\", labels.shape)\n",
    "\n",
    "# Get the vocabulary size\n",
    "vocab_size = len(tokenizer_arabic.word_index) + 1  # Adding 1 because of reserved 0 index\n",
    "\n",
    "kf = KFold(n_splits= 2)\n",
    "fold_no = 1\n",
    "for train, test in kf.split(padded_sequences_arabic, labels):\n",
    "    # Create the transformer model for text classification\n",
    "    model = transformer_classifier(max_sequence_length, vocab_size, num_classes)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = Adam(learning_rate=1e-4, clipvalue=0.5)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[AUC(curve='PR'), 'accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(padded_sequences_arabic, labels, epochs=10, batch_size=8, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9590290784835815\n"
     ]
    }
   ],
   "source": [
    "# show the accuracy of the model\n",
    "loss, accuracy, _ = model.evaluate(padded_sequences_arabic, labels, verbose=False)\n",
    "\n",
    "print(\"Training Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Load the preprocessed test data\n",
    "test_df = pd.read_csv('preprocessed_test.csv')\n",
    "\n",
    "# Drop rows with NaN values in the 'preprocessed_review' column\n",
    "test_df = test_df.dropna(subset=['preprocessed_review'])\n",
    "\n",
    "# Tokenize the preprocessed reviews in the test data using the same tokenizer\n",
    "sequences_test = tokenizer_arabic.texts_to_sequences(test_df['preprocessed_review'])\n",
    "padded_sequences_test = pad_sequences(sequences_test, maxlen=max_sequence_length)\n",
    "\n",
    "# Predict using the model on the test data\n",
    "predictions = model.predict(padded_sequences_test)\n",
    "predicted_ratings = np.argmax(predictions, axis=1) - 1\n",
    "\n",
    "# Add predicted ratings as a new column in the test data\n",
    "test_df['rating'] = predicted_ratings\n",
    "\n",
    "# Save the test data with predicted ratings as a CSV file\n",
    "test_df.to_csv('predicted_test_transformer.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "877\n"
     ]
    }
   ],
   "source": [
    "df_lstm = pd.read_csv('predicted_test.csv')\n",
    "df_transformer = pd.read_csv('predicted_test_transformer.csv')\n",
    "\n",
    "df = (df_lstm['rating'] == df_transformer['rating'])\n",
    "print(df.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
